---
title: >-
  How DeepSeek's new way to train advanced AI models could disrupt everything -
  again
date: '2026-01-02'
excerpt: >-
  Innovation Home Innovation Artificial Intelligence How DeepSeek's new way to
  train advanced AI models could disrupt everything - again The Chinese AI...
coverImage: >-
  https://images.unsplash.com/photo-1677442136019-21780ecad995?w=400&h=200&fit=crop&auto=format
author: AIVibe
tags:
  - Ai
  - Openai
  - Llm
  - Work
category: Work
source: 'https://www.zdnet.com/article/deepseek-research-training-models/'
---
Innovation      
      Home
    
      Innovation
    
      Artificial Intelligence
       
    How DeepSeek's new way to train advanced AI models could disrupt everything - again
     
    The Chinese AI lab may have just found an approach to training frontier LLMs that's both practical and scalable, even for more cash-strapped developers.
      Written by 
            Webb Wright, Contributing WriterContributing Writer  Jan. 2, 2026 at 12:25 p.m. PT                           Flavio Coelho/	Moment via GettyFollow ZDNET: Add us as a preferred source on Google.ZDNET's key takeawaysDeepSeek debuted Manifold-Constrained Hyper-Connections, or mHCs.They offer a way to scale LLMs without incurring huge costs.The company postponed the release of its R2 model in mid-2025.Just before the start of the new year, the AI world was introduced to a potential game-changing new method for training advanced models.A team of researchers from Chinese AI firm DeepSeek released a paper on Wednesday outlining what it called Manifold-Constrained Hyper-Connections, or mHC for short, which may provide a pathway for engineers to build and scale large language models without the huge computational costs that are typically required.Also: Is DeepSeek's new model the latest blow to proprietary AI?DeepSeek leapt into the cultural spotlight one year ago with its release of R1, a model that rivaled the capabilities of OpenAI's o1 and that was reportedly trained at a fraction of the cost. The release came as a shock to US-based tech developers, because it showed that access to huge reserves of capital and computing resources wasn't necessarily required to train cutting-edge AI models. The new mHC paper could turn out to be the technological framework for DeepSeek's forthcoming model, R2, which was expected in the middle of last year but was postponed, reportedly due to China's limited access to advanced AI chips and to concerns from the company's CEO Liang Wenfeng about the model's performance.The c
