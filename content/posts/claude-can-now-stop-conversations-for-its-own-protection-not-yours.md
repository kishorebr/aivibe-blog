---
title: 'Claude can now stop conversations - for its own protection, not yours'
date: '2025-08-18'
excerpt: >-
  Innovation Home Innovation Artificial Intelligence Claude can now stop
  conversations - for its own protection, not yours The company has given its
  AI...
coverImage: >-
  https://images.unsplash.com/photo-1677442136019-21780ecad995?w=400&h=200&fit=crop&auto=format
author: AIVibe
tags:
  - Ai
category: General AI
source: >-
  https://www.zdnet.com/article/claude-can-now-stop-conversations-for-its-own-protection-not-yours/
---
Innovation      
      Home
    
      Innovation
    
      Artificial Intelligence
       
    Claude can now stop conversations - for its own protection, not yours
     
    The company has given its AI chatbot the ability to end toxic conversations as part of its broader 'model welfare' initiative.
      Written by 
            Webb Wright, Contributing WriterContributing Writer  Aug. 18, 2025 at 11:13 a.m. PT                           CHRISTOPH BURGSTEDT/SCIENCE PHOTO LIBRARY via Getty ImagesZDNET's key takeaways:Claude Opus 4 and 4.1 can now end some "potentially distressing" conversations.It will activate only in some cases of persistent user abuse.The feature is geared toward protecting models, not users. Anthropic's Claude chatbot can now end some conversations with human users who are abusing or misusing the chatbot, the company announced on Friday. The new feature is integrated with Claude Opus 4 and Opus 4.1. Also: Claude can teach you how to code now, and more - how to try itClaude will only exit chats with users in extreme edge cases, after "multiple attempts at redirection have failed and hope of a productive interaction has been exhausted," Anthropic noted. "The vast majority of users will not notice or be affected by this feature in any normal product use, even when discussing highly controversial issues with Claude."If Claude ends a conversation, the user will no longer be able to send messages in that particular thread; all of their other conversations, however, will remain open and unaffected. Importantly, users who Claude ends chats with will not experience penalties or delays in starting new conversations immediately. They will also be able to return to and retry previous chats "to create new branches of ended conversations," Anthropic said. The chatbot is designed not to end conversations with users who are perceived as being at risk of harming themselves or others.Tracking AI model well-being The feature isn't aimed at improving user safety -
