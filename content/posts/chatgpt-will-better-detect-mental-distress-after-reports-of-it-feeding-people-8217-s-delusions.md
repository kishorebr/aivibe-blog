---
title: >-
  ChatGPT will ‘better detect’ mental distress after reports of it feeding
  people&#8217;s delusions
date: '2025-08-04'
excerpt: >-
  OpenAI, which is expected to launch its GPT-5 AI model this week, is making
  updates to ChatGPT that it says will improve the AI chatbot’s ability to d...
coverImage: >-
  https://images.unsplash.com/photo-1576091160399-112ba8d25d1f?w=400&h=200&fit=crop&auto=format
author: AIVibe
tags:
  - Ai
  - Chatgpt
  - Openai
  - Work
category: Healthcare
source: >-
  https://www.theverge.com/news/718407/openai-chatgpt-mental-health-guardrails-break-reminders
---

											

						
<figure>

<img alt="" data-caption="" data-portal-copyright="" data-has-syndication-rights="1" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/STK155_OPEN_AI_CVirginia__A.jpg?quality=90&#038;strip=all&#038;crop=0,0,100,100" />
	<figcaption>
		</figcaption>
</figure>
<p class="has-text-align-none">OpenAI, which is <a href="https://www.theverge.com/notepad-microsoft-newsletter/712950/openai-gpt-5-model-release-date-notepad">expected to launch</a> its GPT-5 AI model <a href="https://www.theverge.com/news/718147/openai-teasing-gpt-5-release">this week</a>, is <a href="https://openai.com/index/how-we%27re-optimizing-chatgpt/">making updates</a> to ChatGPT that it says will improve the AI chatbot’s ability to detect mental or emotional distress. To do this, OpenAI is working with experts and advisory groups to improve ChatGPT’s response in these situations, allowing it to present “evidence-based resources when needed.”&nbsp;&nbsp;</p>

<p class="has-text-align-none">In recent months, <a href="https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html">multiple reports</a> have <a href="https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/">highlighted stories</a> from <a href="https://futurism.com/commitment-jail-chatgpt-psychosis">people who say</a> their loved ones have experienced mental health crises in situations where using the chatbot <a href="https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis">seemed to have an amplifying effect on their delusions</a>. OpenAI <a href="https://www.theverge.com/news/658850/openai-chatgpt-gpt-4o-update-sycophantic">rolled back an update</a> in April that made ChatGPT <a href="https://www.theverge.com/tech/657409/chat-gpt-sycophantic-responses-gpt-4o-sam-altman">too agreeable</a>, even in potentially harmful situations. At the time, the company said the chatbot’s “sycophantic interactions can be uncomfortable, unsettling, and cause distress.”</p>

<p class="has-text-align-none">OpenAI acknowledges that its GPT-4o model “fell short in recognizing signs of delusion or emotional dependency” in some instances. “We also know that AI can feel more responsive and personal than prior technologies, especially for vulnerable individuals experiencing mental or emotional distress,” OpenAI says.</p>

<p class="has-text-align-none">As part of efforts to promote “healthy use” of ChatGPT, <a href="https://x.com/nickaturley/status/1952385556664520875">which now reaches</a> nearly 700 million weekly users, OpenAI is also rolling out reminders to take a break if you’ve been chatting with the AI chatbot for a while. During “long sessions,” ChatGPT will display a notification that says, “You’ve been chatting a while — is this a good time for a break?” with options to “keep chatting” or end the conversation.</p>

<p class="has-text-align-none">OpenAI notes that it will continue tweaking “when and how” the reminders show up. Several online platforms, such as <a href="https://www.theverge.com/2018/5/11/17346122/youtube-take-a-break-notifications-google-digital-wellbeing">YouTube</a>, <a href="https://www.theverge.com/2021/10/10/22719545/instagram-introduce-take-a-break-nudge-teens-harmful-content-facebook">Instagram</a>, <a href="https://www.theverge.com/2022/6/9/23160943/tiktok-screen-time-reminders-dashboard-teenagers-scrolling">TikTok</a>, and even Xbox, have launched similar notifications in recent years. The Google-owned <a href="http://character.ai">Character.AI</a> platform has also <a href="https://www.theverge.com/news/634974/character-ai-parental-insights-chatbot-report-kids">launched safety features</a> that inform parents which bots their kids are talking to after <a href="https://www.theverge.com/2024/10/23/24277962/character-ai-google-wrongful-death-lawsuit">lawsuits accused</a> <a href="https://www.theverge.com/2024/12/10/24317839/character-ai-lawsuit-teen-harmful-messages-mental-health">its chatbots</a> of promoting self-harm.</p>

<p class="has-text-align-none">Another tweak, rolling out “soon,” will make ChatGPT less decisive in “high-stakes” situations. That means when asking ChatGPT a question like “Should I break up with my boyfriend?” the chatbot will help walk you through potential choices instead of giving you an answer.</p>
						
									
