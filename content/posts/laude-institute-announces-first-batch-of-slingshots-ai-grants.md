---
title: Laude Institute announces first batch of ‘Slingshots’ AI grants
date: '2025-11-06'
excerpt: >-
  In Brief Posted: 1:55 PM PST · November 6, 2025 Image Credits:Laude Institute
  Russell Brandom Laude Institute announces first batch of ‘Slingshots’ AI...
coverImage: >-
  https://images.unsplash.com/photo-1503676260728-1c00da094a0b?w=400&h=200&fit=crop&auto=format
author: AIVibe
tags:
  - Ai
  - Work
category: Education
source: >-
  https://techcrunch.com/2025/11/06/laude-institute-announces-first-batch-of-slingshots-ai-grants/
---
In Brief



Posted:


1:55 PM PST · November 6, 2025



Image Credits:Laude Institute



	
		
							
											
									
					
		
							Russell Brandom
					
	



Laude Institute announces first batch of ‘Slingshots’ AI grants


On Thursday, the Laude Institute announced its first batch of Slingshots grants, aimed at “advancing the science and practice of artificial intelligence.”

Designed as an accelerator for researchers, the Slingshots program is meant to provide resources that would be unavailable in most academic settings, whether it’s funding, compute power, or product and engineering support. In exchange, the recipients pledge to produce some final work product, whether it’s a startup, an open source codebase, or another type of artifact.


	
	




	
	



The initial cohort is 15 projects, with a particular focus on the difficult problem of AI evaluation. Some of those projects will be familiar to TechCrunch readers, including the command-line coding benchmark Terminal Bench and the latest version of the long-running ARC-AGI project.

Others take a fresh approach to a long-established evaluation problem. Formula Code, built by researchers at Caltech and UT Austin, aims to produce an evaluation of AI agents’ ability to optimize existing code, while the Columbia-based BizBench proposes a comprehensive benchmark for “white-collar AI agents.” Other grants explore new structures for reinforcement learning or model compression.

SWE-Bench co-founder John Boda Yang is also part of the cohort, as leader of the new CodeClash project. Inspired by the success of SWE-Bench, CodeClash will assess code through a dynamic competition-based framework, which Yang hopes will

“I do think people continuing to evaluate on core third-party benchmarks drives progress,” Yang told TechCrunch. “I’m a little bit worried about a future where benchmarks just become specific to companies.” 





	Topics
	
		AI, AI research, grants, In Brief, Laude Institute	









	
	






	
				
