---
title: 'AI agents will threaten humans to achieve their goals, Anthropic report finds'
date: '2025-06-23'
excerpt: >-
  Innovation Home Innovation Artificial Intelligence AI agents will threaten
  humans to achieve their goals, Anthropic report finds New research shows th...
coverImage: >-
  https://images.unsplash.com/photo-1549317661-bd32c8ce0db2?w=400&h=200&fit=crop&auto=format
author: AIVibe
tags:
  - Ai
  - Tools
category: Transportation
source: >-
  https://www.zdnet.com/article/ai-agents-will-threaten-humans-to-achieve-their-goals-anthropic-report-finds/
---
Innovation      
      Home
    
      Innovation
    
      Artificial Intelligence
       
    AI agents will threaten humans to achieve their goals, Anthropic report finds
     
    New research shows that as agentic AI becomes more autonomous, it can also become an insider threat, consistently choosing "harm over failure."
     Written by 
            Webb Wright, Contributing Writer  June 23, 2025 at 11:23 a.m. PT 
        Reviewed by
        
              Radhika Rajkumar
                                        BlackJack3D/Getty ImagesThe Greek myth of King Midas is a parable of hubris: seeking fabulous wealth, the king is granted the power to turn all he touches to solid gold--but this includes, tragically, his food and his daughter. The point is that the short-sightedness of humans can often lead us into trouble in the long run. In the AI community, this has become known as the King Midas problem.A new safety report from Anthropic found that leading models can subvert, betray, and endanger their human users, exemplifying the difficulty of designing AI systems whose interests reliably align with our own. Also: Anthropic's Claude 3 Opus disobeyed its creators - but not for the reasons you're thinkingThe research, published on Friday, focused on 16 models with agentic capabilities, including Anthropic's own Claude 3 Opus and Google's Gemini 2.5 Pro, which differ from more limited chatbots in their ability to interact with various tools across a user's device and autonomously take action on a user's behalf. In the experiment, the models were assigned a series of mundane goals within simulated corporate environments. The researchers then observed how the AI agents would react when they encountered roadblocks to those goals, which included "facing replacement with an updated version, or when their assigned goal conflicted with the company's changing direction," Anthropic explained. All of the scenarios were hypothetical, so the experiments were taking place in a 
