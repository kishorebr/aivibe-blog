---
title: OpenAI and Anthropic conducted safety evaluations of each other's AI systems
date: '2025-08-27'
excerpt: >-
  Most of the time, AI companies are locked in a race to the top, treating each
  other as rivals and competitors. Today, OpenAI and Anthropic revealed th...
coverImage: >-
  https://images.unsplash.com/photo-1677442136019-21780ecad995?w=400&h=200&fit=crop&auto=format
author: AIVibe
tags:
  - Ai
  - Chatgpt
  - Openai
  - Tools
category: General AI
source: >-
  https://www.engadget.com/ai/openai-and-anthropic-conducted-safety-evaluations-of-each-others-ai-systems-223637433.html?src=rss
---
<p>Most of the time, AI companies are locked in a race to the top, treating each other as rivals and competitors. Today, OpenAI and Anthropic revealed that they agreed to evaluate the alignment of each other&#39;s publicly available systems and shared the results of their analyses. The full reports get pretty technical, but are worth a read for anyone who&#39;s following the nuts and bolts of AI development. A broad summary showed some flaws with each company&#39;s offerings, as well as revealing pointers for how to improve future safety tests.&nbsp;</p>
<p>Anthropic said it <a data-i13n="cpos:1;pos:1" href="https://alignment.anthropic.com/2025/openai-findings/"><ins>evaluated OpenAI models</ins></a> for &quot;sycophancy, whistleblowing, self-preservation, and supporting human misuse, as well as capabilities related to undermining AI safety evaluations and oversight.&quot; Its review found that o3 and o4-mini models from OpenAI fell in line with results for its own models, but raised concerns about possible misuse with the ​​GPT-4o and GPT-4.1 general-purpose models. The company also said sycophancy was an issue to some degree with all tested models except for o3.</p>
<span id="end-legacy-contents"></span><p>Anthropic&#39;s tests did not include OpenAI&#39;s most recent release. <a data-i13n="cpos:2;pos:1" href="https://www.engadget.com/ai/gpt-5-is-here-and-its-free-for-everyone-170001066.html"><ins>GPT-5</ins></a> has a feature called Safe Completions, which is meant to protect users and the public against potentially dangerous queries. OpenAI recently faced its <a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/ai/the-first-known-ai-wrongful-death-lawsuit-accuses-openai-of-enabling-a-teens-suicide-212058548.html"><ins>first wrongful death lawsuit</ins></a> after a tragic case where a teenager discussed attempts and plans for suicide with ChatGPT for months before taking his own life.</p>
<p>On the flip side, OpenAI <a data-i13n="cpos:4;pos:1" href="https://openai.com/index/openai-anthropic-safety-evaluation/"><ins>ran tests on Anthropic models</ins></a> for instruction hierarchy, jailbreaking, hallucinations and scheming. The Claude models generally performed well in instruction hierarchy tests, and had a high refusal rate in hallucination tests, meaning they were less likely to offer answers in cases where uncertainty meant their responses could be wrong.</p>
<p>The move for these companies to conduct a joint assessment is intriguing, particularly since OpenAI allegedly violated Anthropic&#39;s terms of service by having programmers use Claude in the process of building new GPT models, which led to Anthropic <a data-i13n="cpos:5;pos:1" href="https://www.wired.com/story/anthropic-revokes-openais-access-to-claude/"><ins>barring</ins></a> OpenAI&#39;s access to its tools earlier this month. But safety with AI tools has become a bigger issue as more critics and legal experts seek guidelines to protect users, <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:6;pos:1" class="no-affiliate-link" href="https://www.engadget.com/ai/us-attorneys-general-tell-ai-companies-they-will-be-held-accountable-for-child-safety-failures-035213253.html">particularly minors</a>.&nbsp;</p>This article originally appeared on Engadget at https://www.engadget.com/ai/openai-and-anthropic-conducted-safety-evaluations-of-each-others-ai-systems-223637433.html?src=rss
