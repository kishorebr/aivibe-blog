---
title: Anthropic says Claude helps emotionally support users - we're not convinced
date: '2025-06-27'
excerpt: >-
  Innovation Home Innovation Artificial Intelligence Anthropic says Claude helps
  emotionally support users - we're not convinced While Anthropic found C...
coverImage: >-
  https://images.unsplash.com/photo-1576091160399-112ba8d25d1f?w=400&h=200&fit=crop&auto=format
author: AIVibe
tags:
  - Ai
  - Openai
category: Healthcare
source: >-
  https://www.zdnet.com/article/anthropic-says-claude-helps-emotionally-support-users-were-not-convinced/
---
Innovation      
      Home
    
      Innovation
    
      Artificial Intelligence
       
    Anthropic says Claude helps emotionally support users - we're not convinced
     
    While Anthropic found Claude doesn't enforce negative outcomes in affective conversations, some researchers question the findings.
     Written by 
            Radhika Rajkumar, Editor  June 27, 2025 at 12:40 p.m. PT                            Richard Drury/Getty ImagesMore and more, in the midst of a loneliness epidemic and structural barriers to mental health support, people are turning to AI chatbots for everything from career coaching to romance. Anthropic's latest study indicates its chatbot, Claude, is handling that well -- but some experts aren't convinced. Also: You shouldn't trust AI for therapy - here's whyOn Thursday, Anthropic published new research on its Claude chatbot's emotional intelligence (EQ) capabilities -- what the company calls affective use, or conversations "where people engage directly with Claude in dynamic, personal exchanges motivated by emotional or psychological needs such as seeking interpersonal advice, coaching, psychotherapy/counseling, companionship, or sexual/romantic roleplay," the company explained. While Claude is designed primarily for tasks like code generation and problem solving, not emotional support, the research acknowledges that this type of use is still happening, and is worthy of investigation given the risks. The company also noted that doing so is relevant to its focus on safety. The main findings Anthropic analyzed about 4.5 million conversations from both Free and Pro Claude accounts, ultimately settling on 131,484 that fit the affective use criteria. Using its privacy data tool Clio, Anthropic stripped conversations of personally identifying information (PII). The study revealed that only 2.9% of Claude interactions were classified as affective conversations, which the company says mirrors previous findings from OpenAI. Examples of 
