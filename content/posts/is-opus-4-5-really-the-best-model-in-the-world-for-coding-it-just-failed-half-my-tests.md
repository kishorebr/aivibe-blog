---
title: >-
  Is Opus 4.5 really 'the best model in the world for coding'? It just failed
  half my tests
date: '2025-11-25'
excerpt: >-
  Innovation Home Innovation Artificial Intelligence Is Opus 4.5 really 'the
  best model in the world for coding'? It just failed half my tests Here's wh...
coverImage: >-
  https://images.unsplash.com/photo-1551434678-e076c223a692?w=400&h=200&fit=crop&auto=format
author: AIVibe
tags:
  - Ai
  - Productivity
  - Work
category: Productivity
source: >-
  https://www.zdnet.com/article/is-opus-4-5-really-the-best-model-in-the-world-for-coding-it-just-failed-half-my-tests/
---
Innovation      
      Home
    
      Innovation
    
      Artificial Intelligence
       
    Is Opus 4.5 really 'the best model in the world for coding'? It just failed half my tests
     
    Here's what happened when I pushed Anthropic's new model through some simple development tasks.
      Written by 
            David Gewirtz, Senior Contributing EditorSenior Contributing Editor  Nov. 24, 2025 at 7:00 p.m. PT                            Screenshot by David Gewirtz/ZDNETFollow ZDNET: Add us as a preferred source on Google.ZDNET's key takeawaysOpus 4.5 failed half my coding tests, despite bold claimsFile handling glitches made basic plugin testing nearly impossibleTwo tests passed, but reliability issues still dominate the storyI've got to tell you: I've had fairly okay coding results with Claude's lower-end Sonnet AI model. But for whatever reason, its high-end Opus model has never done well on my tests.Usually, you expect the super-duper coding model to code better than the cheap seats, but with Opus, not so much. Also: Google's Antigravity puts coding productivity before AI hype - and the result is astonishingNow, we're back with Opus 4.5. Anthropic, the company behind Claude claims, and I quote, "Our newest model, Claude Opus 4.5, is available today. It's intelligent, efficient, and the best model in the world for coding, agents, and computer use." The best model in the world for coding? No, it's not. At least not yet.Those of you who've been following along know that I have a standard set of four fairly low-end coding tests I put the AI models through on a regular basis. They test a bunch of very simple skills and framework knowledge, but they can sometimes trip up the AIs. Also: How I test an AI chatbot's coding ability - and you can, tooI'll give you the TL;DR right now. Opus 4.5 crashed and burned on one test, turned in a mediocre and not-quite-good-enough answer on the second, and passed the remaining two. With a 50% score, we're definitely not lookin
