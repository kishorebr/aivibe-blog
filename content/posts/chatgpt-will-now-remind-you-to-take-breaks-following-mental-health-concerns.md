---
title: 'ChatGPT will now remind you to take breaks, following mental health concerns'
date: '2025-08-04'
excerpt: >-
  OpenAI has announced that ChatGPT will now remind users to take breaks if
  they&#39;re in a particularly long chat with AI. The new feature is part of...
coverImage: >-
  https://images.unsplash.com/photo-1576091160399-112ba8d25d1f?w=400&h=200&fit=crop&auto=format
author: AIVibe
tags:
  - Ai
  - Chatgpt
  - Openai
category: Healthcare
source: >-
  https://www.engadget.com/ai/chatgpt-will-now-remind-you-to-take-breaks-following-mental-health-concerns-180221008.html?src=rss
---
<p>OpenAI has announced that ChatGPT will now <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:1;pos:1" class="no-affiliate-link" href="https://openai.com/index/how-we&#39;re-optimizing-chatgpt/">remind users to take breaks</a> if they&#39;re in a particularly long chat with AI. The new feature is part of OpenAI&#39;s ongoing attempts to get users to cultivate a healthier relationship with the frequently compliant and overly-encouraging AI assistant.</p>
<p>The company&#39;s announcement suggests the &quot;gentle reminders&quot; will appear as pop-ups in chats that users will have to click or tap through to continue using ChatGPT. &quot;Just Checking In,&quot; OpenAI&#39;s sample pop-up reads. &quot;You&#39;ve been chatting for a while — is this a good time for a break?&quot; The system is reminiscent of the reminders some Nintendo Wii and Switch games will show you if you play for an extended period of time, though there&#39;s an unfortunately dark context to the ChatGPT feature.</p>
<span id="end-legacy-contents"></span><p>The &quot;yes, and&quot; quality of OpenAI&#39;s AI and it&#39;s ability to hallucinate factually incorrect or dangerous responses has led users down dark paths, <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:2;pos:1" class="no-affiliate-link" href="https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html"><em>The New York Times </em>reported in June</a> — including suicidal ideation. Some of the users whose delusions ChatGPT indulged already had a history of mental illness, but the chatbot still did a bad job of consistently shutting down unhealthy conversations. OpenAI acknowledges some of those shortcomings in its blog post, and says that ChatGPT will be updated in the future to respond more carefully to &quot;high-stakes personal decisions.&quot; Rather than provide a direct answer, the company says the chatbot will help users think through problems, offer up questions and list pros and cons.</p>
<p>OpenAI obviously wants ChatGPT to feel helpful, encouraging and enjoyable to use, but it&#39;s not hard to package those qualities into an AI that&#39;s sycophantic. The company was forced to <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:3;pos:1" class="no-affiliate-link" href="https://www.engadget.com/ai/openai-rolls-back-update-that-made-chatgpt-an-ass-kissing-weirdo-203056185.html">rollback an update to ChatGPT in April</a> that lead the chatbot to respond in ways that were annoying and overly-agreeable. Taking breaks from ChatGPT — and having the AI <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:4;pos:1" class="no-affiliate-link" href="https://www.engadget.com/ai/openais-operator-can-surf-the-web-for-you-210029243.html">do things without your active participation</a> — will make issues like that less visible. Or, at the very least, it&#39;ll give users time to check whether the answers ChatGPT is providing are even correct.</p>This article originally appeared on Engadget at https://www.engadget.com/ai/chatgpt-will-now-remind-you-to-take-breaks-following-mental-health-concerns-180221008.html?src=rss
