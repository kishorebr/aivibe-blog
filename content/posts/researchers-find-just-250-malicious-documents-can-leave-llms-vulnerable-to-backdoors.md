---
title: >-
  Researchers find just 250 malicious documents can leave LLMs vulnerable to
  backdoors
date: '2025-10-09'
excerpt: >-
  Artificial intelligence companies have been working at breakneck speeds to
  develop the best and most powerful tools, but that rapid development hasn&#...
coverImage: >-
  https://images.unsplash.com/photo-1677442136019-21780ecad995?w=400&h=200&fit=crop&auto=format
author: AIVibe
tags:
  - Ai
  - Llm
  - Work
  - Tools
category: Work
source: >-
  https://www.engadget.com/researchers-find-just-250-malicious-documents-can-leave-llms-vulnerable-to-backdoors-191112960.html?src=rss
---
<p style="text-align:left;"><span style="color:rgb(0, 0, 0);font-family:Arial, sans-serif;">Artificial intelligence companies have been working at breakneck speeds to develop the best and most powerful tools, but that rapid development hasn&#39;t always been coupled with clear understandings of AI&#39;s limitations or weaknesses. Today, Anthropic released a </span><a target="_blank" class="link" href="https://www.anthropic.com/research/small-samples-poison" data-i13n="cpos:1;pos:1"><span style="color:rgb(17, 85, 204);font-family:Arial, sans-serif;">report</span></a><span style="color:rgb(0, 0, 0);font-family:Arial, sans-serif;"> on how attackers can influence the development of a large language model.</span></p><p style="text-align:left;"><span style="color:rgb(0, 0, 0);font-family:Arial, sans-serif;">The study centered on a type of attack called poisoning, where an LLM is pretrained on malicious content intended to make it learn dangerous or unwanted behaviors. The key finding from this study is that a bad actor doesn&#39;t need to control a percentage of the pretraining materials to get the LLM to be poisoned. Instead, the researchers found that a small and fairly constant number of malicious documents can poison an LLM, regardless of the size of the model or its training materials. The study was able to successfully backdoor LLMs based on using only 250 malicious documents in the pretraining data set, a much smaller number than expected for models ranging from 600 million to 13 billion parameters.&nbsp;</span></p><p style="text-align:left;"><span style="color:rgb(0, 0, 0);font-family:Arial, sans-serif;">&quot;Weâ€™re sharing these findings to show that data-poisoning attacks might be more practical than believed, and to encourage further research on data poisoning and potential defenses against it,&quot; the company said. Anthropic collaborated with the UK AI Security Institute and the Alan Turing Institute on the research.</span></p>This article originally appeared on Engadget at https://www.engadget.com/researchers-find-just-250-malicious-documents-can-leave-llms-vulnerable-to-backdoors-191112960.html?src=rss
