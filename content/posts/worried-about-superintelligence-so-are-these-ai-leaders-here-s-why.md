---
title: Worried about superintelligence? So are these AI leaders - here's why
date: '2025-10-22'
excerpt: >-
  Innovation Home Innovation Artificial Intelligence Worried about
  superintelligence? So are these AI leaders - here's why Leaders including
  Geoffrey Hi...
coverImage: >-
  https://images.unsplash.com/photo-1677442136019-21780ecad995?w=400&h=200&fit=crop&auto=format
author: AIVibe
tags:
  - Ai
  - Chatgpt
category: Work
source: >-
  https://www.zdnet.com/article/worried-about-superintelligence-so-are-these-ai-leaders-heres-why/
---
Innovation      
      Home
    
      Innovation
    
      Artificial Intelligence
       
    Worried about superintelligence? So are these AI leaders - here's why
     
    Leaders including Geoffrey Hinton and Yoshua Bengio signed a petition arguing that the next phase of AI poses an existential risk to humanity.
      Written by 
            Webb Wright, Contributing WriterContributing Writer  Oct. 22, 2025 at 11:52 a.m. PT                            MirageC/Moment via Getty ImagesFollow ZDNET: Add us as a preferred source on Google.ZDNET's key takeawaysLeaders argue that AI could existentially threaten humans.Prominent AI figures, alongside 1,300 others, endorsed the worry.The public is equally concerned about "superintelligence." The surprise release of ChatGPT just under three years ago was the starting gun for an AI race that has been rapidly accelerating ever since. Now, a group of industry experts is warning -- and not for the first time -- that AI labs should slow down before humanity drives itself off a cliff.Also: What Bill Gates really said about AI replacing coding jobsA statement published Wednesday by the Future of Life Institute (FLI), a nonprofit organization focused on existential AI risk, argues that the development of "superintelligence" -- an AI industry buzzword that usually refers to a hypothetical machine intelligence that can outperform humans on any cognitive task -- presents an existential risk and should therefore be halted until a safe pathway forward can be established.A stark warning The unregulated competition among leading AI labs to build superintelligence could result in "human economic obsolescence and disempowerment, losses of freedom, civil liberties, dignity, and control, to national security risks and even potential human extinction," the authors of the statement wrote. They go on to argue that a prohibition on the development of superintelligent machines could be enacted until there is (1) "broad scientific consensus that
